# Optimizing an ML Pipeline in Azure
## Overview
This project is part of the Udacity Azure ML Nanodegree. In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model. This model is then compared to an Azure AutoML run.

## Summary
This dataset contains data about a Telemarketing strategy implemented by a bank. The aim is to predict if a client would subscribe to a term deposit.

The best performing model is **VotingEnsemble Classifier** with an accuracy of 91.71%.

## Scikit-learn Pipeline
The dataset contains data about a client, details regarding the last contact with that client and the socio-economic conditions at that time. All categorical attributes were one-hot encoded.

The model used in the pipeline is Logistic Regression. 

The hyperparameters no_of_iterations and C(Inverse of regularization strength) were optimized using Hyperdrive. The parameter sampler used is RandomParameterSampling. As this sampler chooses a value for each of parameters randomly, after a certain number of runs all combinations will be tried out and the best one can be chosen.

The early stopping policy used is BanditPolicy. Its parameters are set such that in every alternate interval the policy is applied. This will terminate runs that are not performing well based on the slack factor. This reduces the wastage of compute resources.

## AutoML
Some of the models generated by AutoML, in the order of accuracy are:
  1. VotingEnsemble Calssifier                     (91.71%) 
  1. XGBoost Classifier (Standard Scalar)          (91.52%)
  1. LightGBM Classifier (Maxabs Scalar)           (91.51%)

## Pipeline comparison
The best model obtained using Hyperdrive is Logistic Regression with an accuracy of 91.35%.
The best model generated by AutoML is Voting Ensemble Calssifier with acurracy of 91.71%.

The slight improvement in accuracy is because Ensemble Classifiers combine multiple models.

## Future work
The dataset is imbalanced. This was also indicated by the Data Gaurdrails in AutoML. This can be addressed by using upsampling or downsampling techniques.
