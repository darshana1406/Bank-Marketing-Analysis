# Optimizing an ML Pipeline in Azure
## Overview
This project is part of the Udacity Azure ML Nanodegree. In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model. This model is then compared to an Azure AutoML run.

## Summary
* This dataset contains data about a Telemarketing strategy implemented by a bank. The aim is to predict if a client would subscribe to a term deposit.

* The best performing model is **VotingEnsemble Classifier** with an accuracy of 91.71%.

## Scikit-learn Pipeline
* The dataset contains data about a client, details regarding the last contact with that client and the socio-economic conditions at that time. All categorical attributes were one-hot encoded.

* The model used in the pipeline is Logistic Regression. 

* The hyperparameters no_of_iterations and C(Inverse of regularization strength) were optimized using Hyperdrive. The parameter sampler used is RandomParameterSampling. As this sampler chooses a value for each of parameters randomly, after a certain number of runs all combinations will be tried out and the best one can be chosen.

* The early stopping policy used is BanditPolicy. Its parameters are set such that in every alternate interval the policy is applied. This will terminate runs that are not performing well based on the slack factor. This reduces the wastage of compute resources.

## AutoML
* Some of the models generated by AutoML, in the order of accuracy are:
  1. VotingEnsemble Calssifier                     (91.71%) 
  1. XGBoost Classifier (Standard Scalar)          (91.52%)
  1. LightGBM Classifier (Maxabs Scalar)           (91.51%)
  
* The hyperparameters of VotingEnsemble Classifier:
  * "ensembled_iterations": "[1, 0, 13, 10, 9, 11, 12]"
  * "ensembled_algorithms": "['XGBoostClassifier', 'LightGBM', 'SGD', 'SGD', 'SGD', 'SGD', 'ExtremeRandomTrees']"
  * "ensemble_weights": "[0.2, 0.4, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.13333333333333333]"

## Pipeline comparison
* The best model obtained using Hyperdrive is Logistic Regression with an accuracy of 91.35%.
* The best model generated by AutoML is Voting Ensemble Calssifier with acurracy of 91.71%.

* The accuracy of both the models are similar, the slight improvement in accuracy is because Ensemble Classifiers combine multiple models.

* In the Scikit-learn Pipeline, a good set of hyperparameters were found for the Logistic Regression Algorithm. If a more powerful algorithm is used, better accuracies can be achieved.

* AutoML generated a variety of models that are very different from Logistic Regression as it has many options for algorithms and feature engineering. It is interesting to note that not all models performed better than Logistic Regression but for huge datasets, AutoML can help in getting started.


## Future work
* The dataset is imbalanced. This was also indicated by the Data Gaurdrails in AutoML. This can be addressed by using upsampling or downsampling techniques.

* In the AutoML configuration, a parameter named featurization is set to 'auto' by default. This means that the a attribute type is automatically detected and feature engineering is performed based on that. This can be improved by defining a custom featurizationConfig object which might improve the quality of the data.

* In the AutoML configuration, enable_DNN parameter can be set to True. Deep Neural Networks might be able to perform better.

* It has been observed that XGBoost Classifier (Standard Scalar)  (91.52%)  also performs well. The hyperparameters of this model can also be optimized using Hyperdrive.

* Finally, running the AutoML experiment for a longer time and enabling early stopping might help in finding better models.
